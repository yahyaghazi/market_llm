"""
Module d'int√©gration avec Ollama pour analyses LLM locales
Supporte DeepSeek-R1 et autres mod√®les Ollama
"""
import requests
import json
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
import logging

from models import ProductAnalysis, MarketAnalysisResult
from config import swot_data, recommendations_data

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class OllamaConfig:
    """Configuration pour Ollama"""
    host: str = "http://localhost:11434"
    model: str = "gemma3:4b"
    temperature: float = 0.7
    top_p: float = 0.9
    max_tokens: Optional[int] = 2000
    timeout: int = 120  # secondes
    
    # Options avanc√©es
    num_ctx: int = 4096  # Taille du contexte
    num_predict: Optional[int] = None  # Alias pour max_tokens
    top_k: int = 40
    repeat_penalty: float = 1.1
    seed: Optional[int] = None
    
    def to_options_dict(self) -> Dict[str, Any]:
        """Convertit la config en dict pour l'API Ollama"""
        options = {
            "temperature": self.temperature,
            "top_p": self.top_p,
            "top_k": self.top_k,
            "repeat_penalty": self.repeat_penalty,
            "num_ctx": self.num_ctx,
        }
        
        if self.max_tokens:
            options["num_predict"] = self.max_tokens
        if self.num_predict:
            options["num_predict"] = self.num_predict
        if self.seed is not None:
            options["seed"] = self.seed
            
        return options


class OllamaClient:
    """Client pour interagir avec Ollama"""
    
    def __init__(self, config: OllamaConfig = None):
        """
        Initialiser le client Ollama
        
        Args:
            config: Configuration Ollama (utilise les valeurs par d√©faut si None)
        """
        self.config = config or OllamaConfig()
        self.base_url = self.config.host
        logger.info(f"ü§ñ OllamaClient initialis√© avec mod√®le: {self.config.model}")
    
    def check_connection(self) -> bool:
        """
        V√©rifie la connexion avec Ollama
        
        Returns:
            bool: True si Ollama est accessible
        """
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                logger.info("‚úÖ Connexion Ollama OK")
                return True
            else:
                logger.error(f"‚ùå Ollama r√©pond avec code {response.status_code}")
                return False
        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå Impossible de se connecter √† Ollama: {e}")
            return False
    
    def list_models(self) -> List[str]:
        """
        Liste les mod√®les disponibles dans Ollama
        
        Returns:
            List[str]: Liste des noms de mod√®les
        """
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=10)
            if response.status_code == 200:
                data = response.json()
                models = [model['name'] for model in data.get('models', [])]
                logger.info(f"üìã Mod√®les disponibles: {', '.join(models)}")
                return models
            else:
                logger.error("‚ùå Impossible de lister les mod√®les")
                return []
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du listing: {e}")
            return []
    
    def check_model_exists(self, model_name: str = None) -> bool:
        """
        V√©rifie si un mod√®le est disponible
        
        Args:
            model_name: Nom du mod√®le (utilise config.model si None)
            
        Returns:
            bool: True si le mod√®le existe
        """
        model = model_name or self.config.model
        models = self.list_models()
        exists = model in models
        
        if not exists:
            logger.warning(f"‚ö†Ô∏è  Mod√®le '{model}' non trouv√©. Mod√®les disponibles: {models}")
        
        return exists
    
    def generate(
        self, 
        prompt: str, 
        system: str = None,
        stream: bool = False
    ) -> Dict[str, Any]:
        """
        G√©n√®re une r√©ponse avec Ollama
        
        Args:
            prompt: Le prompt utilisateur
            system: Prompt syst√®me (optionnel)
            stream: Streaming activ√© (non impl√©ment√© pour l'instant)
            
        Returns:
            Dict contenant la r√©ponse et les m√©tadonn√©es
        """
        url = f"{self.base_url}/api/generate"
        
        payload = {
            "model": self.config.model,
            "prompt": prompt,
            "stream": False,  # Forcer False pour simplifier
            "options": self.config.to_options_dict()
        }
        
        if system:
            payload["system"] = system
        
        logger.info(f"üöÄ G√©n√©ration avec {self.config.model}...")
        logger.debug(f"Prompt: {prompt[:100]}...")
        
        try:
            response = requests.post(
                url, 
                json=payload, 
                timeout=self.config.timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                logger.info("‚úÖ G√©n√©ration r√©ussie")
                logger.debug(f"R√©ponse: {result.get('response', '')[:100]}...")
                return result
            else:
                logger.error(f"‚ùå Erreur Ollama: {response.status_code}")
                logger.error(f"D√©tails: {response.text}")
                return {"error": f"Status {response.status_code}", "response": ""}
                
        except requests.exceptions.Timeout:
            logger.error(f"‚ùå Timeout apr√®s {self.config.timeout}s")
            return {"error": "Timeout", "response": ""}
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la g√©n√©ration: {e}")
            return {"error": str(e), "response": ""}
    
    def chat(
        self, 
        messages: List[Dict[str, str]]
    ) -> Dict[str, Any]:
        """
        Utilise l'API chat d'Ollama
        
        Args:
            messages: Liste de messages [{"role": "user", "content": "..."}]
            
        Returns:
            Dict contenant la r√©ponse
        """
        url = f"{self.base_url}/api/chat"
        
        payload = {
            "model": self.config.model,
            "messages": messages,
            "stream": False,
            "options": self.config.to_options_dict()
        }
        
        logger.info(f"üí¨ Chat avec {self.config.model}...")
        
        try:
            response = requests.post(
                url, 
                json=payload, 
                timeout=self.config.timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                logger.info("‚úÖ Chat r√©ussi")
                return result
            else:
                logger.error(f"‚ùå Erreur chat: {response.status_code}")
                return {"error": f"Status {response.status_code}", "message": {}}
                
        except Exception as e:
            logger.error(f"‚ùå Erreur chat: {e}")
            return {"error": str(e), "message": {}}


class PromptTemplates:
    """Templates de prompts pour diff√©rentes analyses"""
    
    @staticmethod
    def product_analysis_prompt(product: str, sector: str) -> str:
        """
        Cr√©e un prompt pour analyser un produit
        
        Args:
            product: Nom du produit
            sector: Secteur d'activit√©
            
        Returns:
            str: Prompt format√©
        """
        return f"""Tu es un expert en analyse de march√© et strat√©gie commerciale.

Analyse le produit suivant:
- **Produit:** {product}
- **Secteur:** {sector}

Fournis une analyse structur√©e au format JSON avec les cl√©s suivantes:

{{
    "market_share": <float entre 5 et 35>,
    "price": <float entre 100 et 2000>,
    "satisfaction": <float entre 3.0 et 4.8>,
    "growth": <float entre -10 et 40>,
    "strengths": [<liste de 3-5 forces COURTES en fran√ßais, 50 caract√®res max par item>],
    "weaknesses": [<liste de 3-4 faiblesses COURTES en fran√ßais, 50 caract√®res max par item>],
    "opportunities": [<liste de 3-5 opportunit√©s COURTES en fran√ßais, 50 caract√®res max par item>],
    "threats": [<liste de 3-4 menaces COURTES en fran√ßais, 50 caract√®res max par item>],
    "positioning": "<description du positionnement strat√©gique, 100-150 caract√®res>",
    "target_audience": "<description du public cible, 100-150 caract√®res>"
}}

IMPORTANT pour SWOT:
- Chaque item doit √™tre COURT et CONCIS (max 50 caract√®res)
- Phrases compl√®tes mais br√®ves
- √âviter les longues descriptions
- Exemple CORRECT: "Prix √©lev√© limitant l'accessibilit√©"
- Exemple INCORRECT: "Prix premium √©lev√© qui limite consid√©rablement l'accessibilit√© pour une grande partie du march√© cible potentiel"

Sois pr√©cis, r√©aliste et base-toi sur des donn√©es de march√© actuelles.
R√©ponds UNIQUEMENT avec le JSON, sans texte avant ou apr√®s.
    """
    @staticmethod
    def executive_summary_prompt(
        products_data: List[Dict], 
        sector: str
    ) -> str:
        """
        Cr√©e un prompt pour le r√©sum√© ex√©cutif
        
        Args:
            products_data: Donn√©es des produits analys√©s
            sector: Secteur d'activit√©
            
        Returns:
            str: Prompt format√©
        """
        products_list = "\n".join([
            f"- {p['name']}: Part de march√© {p['market_share']:.1f}%, "
            f"Satisfaction {p['satisfaction']:.1f}/5, "
            f"Croissance {p['growth']:+.1f}%"
            for p in products_data
        ])
        
        return f"""Tu es un consultant senior en strat√©gie.

R√©dige un r√©sum√© ex√©cutif professionnel (150-200 mots MAXIMUM) pour une √©tude de march√© du secteur **{sector}**.

Donn√©es des produits analys√©s:
{products_list}

Le r√©sum√© doit:
1. Faire 150-200 mots MAXIMUM
2. √ätre en fran√ßais professionnel et fluide
3. Mentionner le leader et sa part de march√©
4. Inclure la satisfaction client moyenne
5. √âvoquer les tendances et opportunit√©s cl√©s

CRITIQUE: 
- NE PAS utiliser de JSON ou code
- NE PAS mettre de balises markdown (pas de ```, pas de **bold**)
- NE PAS mettre de titre
- R√©diger en TEXTE BRUT continu
- UN SEUL paragraphe fluide (maximum 2 paragraphes)

Commence directement par: "Le secteur {sector}..."
    """
    @staticmethod
    def recommendations_prompt(
        products_data: List[Dict], 
        sector: str
    ) -> str:
        """
        Cr√©e un prompt pour les recommandations strat√©giques
        
        Args:
            products_data: Donn√©es des produits
            sector: Secteur d'activit√©
            
        Returns:
            str: Prompt format√©
        """
        return f"""Tu es un consultant en strat√©gie d'entreprise.

Fournis 6 recommandations strat√©giques pour le secteur **{sector}**.

Contexte:
- Nombre de produits analys√©s: {len(products_data)}
- Secteur: {sector}

Fournis la liste au format JSON:
{{
    "recommendations": [
        "Recommandation 1 (30-60 mots)",
        "Recommandation 2 (30-60 mots)",
        "Recommandation 3 (30-60 mots)",
        "Recommandation 4 (30-60 mots)",
        "Recommandation 5 (30-60 mots)",
        "Recommandation 6 (30-60 mots)"
    ]
}}

Les recommandations doivent √™tre:
- Concr√®tes et actionnables
- Sp√©cifiques au secteur
- Professionnelles
- En fran√ßais

R√©ponds UNIQUEMENT avec le JSON."""
    
    @staticmethod
    def system_prompt() -> str:
        """Prompt syst√®me pour tous les appels"""
        return """Tu es un expert en analyse de march√© et strat√©gie commerciale avec 15 ans d'exp√©rience. 
Tu fournis des analyses pr√©cises, factuelles et professionnelles bas√©es sur des donn√©es de march√© r√©elles.
Tu r√©ponds toujours en fran√ßais et au format demand√© (JSON ou texte selon les instructions)."""


class OllamaMarketAnalyzer:
    """
    Analyseur de march√© utilisant Ollama (LLM local)
    Compatible avec DeepSeek-R1, Llama3, Mistral, etc.
    """
    
    def __init__(
        self, 
        ollama_config: OllamaConfig = None,
        fallback_to_simulation: bool = True
    ):
        """
        Initialiser l'analyseur Ollama
        
        Args:
            ollama_config: Configuration Ollama personnalis√©e
            fallback_to_simulation: Utiliser simulation si Ollama indisponible
        """
        self.config = ollama_config or OllamaConfig()
        self.client = OllamaClient(self.config)
        self.fallback = fallback_to_simulation
        self.prompt_templates = PromptTemplates()
        
        # V√©rifier la connexion
        if not self.client.check_connection():
            logger.warning("‚ö†Ô∏è  Ollama non accessible")
            if not self.fallback:
                raise ConnectionError("Ollama non accessible et fallback d√©sactiv√©")
        
        # V√©rifier le mod√®le
        if not self.client.check_model_exists():
            logger.warning(f"‚ö†Ô∏è  Mod√®le {self.config.model} non trouv√©")
            if not self.fallback:
                raise ValueError(f"Mod√®le {self.config.model} non disponible")
        
        logger.info(f"‚úÖ OllamaMarketAnalyzer initialis√©")
        logger.info(f"   Mod√®le: {self.config.model}")
        logger.info(f"   Temperature: {self.config.temperature}")
        logger.info(f"   Top-P: {self.config.top_p}")
        logger.info(f"   Max tokens: {self.config.max_tokens}")
    
    def analyze_products(
        self, 
        products: List[str], 
        sector: str
    ) -> MarketAnalysisResult:
        """
        Analyse plusieurs produits avec Ollama
        
        Args:
            products: Liste des noms de produits
            sector: Secteur d'activit√©
            
        Returns:
            MarketAnalysisResult: Analyse compl√®te
        """
        from datetime import datetime
        
        logger.info(f"\n{'='*70}")
        logger.info(f"üîç ANALYSE OLLAMA - {len(products)} produits")
        logger.info(f"{'='*70}")
        
        # Analyser chaque produit
        analyses = []
        for i, product in enumerate(products, 1):
            logger.info(f"\nüìä Analyse {i}/{len(products)}: {product}")
            analysis = self._analyze_single_product(product, sector)
            analyses.append(analysis)
        
        # G√©n√©rer r√©sum√© et recommandations
        logger.info(f"\nüìù G√©n√©ration du r√©sum√© ex√©cutif...")
        summary = self._generate_executive_summary(analyses, sector)
        
        logger.info(f"\nüí° G√©n√©ration des recommandations...")
        recommendations = self._generate_recommendations(analyses, sector)
        
        logger.info(f"\n{'='*70}")
        logger.info(f"‚úÖ ANALYSE TERMIN√âE")
        logger.info(f"{'='*70}\n")
        
        return MarketAnalysisResult(
            sector=sector,
            analysis_date=datetime.now().strftime('%d/%m/%Y'),
            products=analyses,
            summary=summary,
            recommendations=recommendations
        )
    
    def _analyze_single_product(
        self, 
        product: str, 
        sector: str
    ) -> ProductAnalysis:
        """
        Analyse un produit avec Ollama
        
        Args:
            product: Nom du produit
            sector: Secteur d'activit√©
            
        Returns:
            ProductAnalysis: Analyse du produit
        """
        # Cr√©er le prompt
        prompt = self.prompt_templates.product_analysis_prompt(product, sector)
        system = self.prompt_templates.system_prompt()
        
        # Appeler Ollama
        result = self.client.generate(prompt, system=system)
        
        # Parser la r√©ponse
        if "error" in result or not result.get("response"):
            logger.warning(f"‚ö†Ô∏è  Erreur LLM, utilisation simulation pour {product}")
            return self._fallback_analysis(product, sector)
        
        try:
            # Extraire le JSON de la r√©ponse
            response_text = result["response"].strip()
            
            # Nettoyer la r√©ponse (parfois le LLM ajoute du texte avant/apr√®s)
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            
            if json_start == -1 or json_end == 0:
                logger.warning("‚ö†Ô∏è  JSON non trouv√© dans la r√©ponse")
                return self._fallback_analysis(product, sector)
            
            json_str = response_text[json_start:json_end]
            data = json.loads(json_str)
            
            # Valider et cr√©er ProductAnalysis
            return ProductAnalysis(
                name=product,
                market_share=float(data.get('market_share', 20.0)),
                price=float(data.get('price', 500.0)),
                satisfaction=float(data.get('satisfaction', 4.0)),
                growth=float(data.get('growth', 10.0)),
                strengths=data.get('strengths', [])[:8],
                weaknesses=data.get('weaknesses', [])[:7],
                opportunities=data.get('opportunities', [])[:8],
                threats=data.get('threats', [])[:7],
                positioning=data.get('positioning', f"Acteur majeur dans {sector}"),
                target_audience=data.get('target_audience', f"Public cible {sector}")
            )
            
        except (json.JSONDecodeError, KeyError, ValueError) as e:
            logger.error(f"‚ùå Erreur parsing JSON: {e}")
            logger.debug(f"R√©ponse brute: {result.get('response', '')[:500]}")
            return self._fallback_analysis(product, sector)
    
    def _generate_executive_summary(
        self, 
        analyses: List[ProductAnalysis], 
        sector: str
    ) -> str:
        """G√©n√®re le r√©sum√© ex√©cutif avec Ollama"""
        products_data = [
            {
                'name': a.name,
                'market_share': a.market_share,
                'satisfaction': a.satisfaction,
                'growth': a.growth
            }
            for a in analyses
        ]
        
        prompt = self.prompt_templates.executive_summary_prompt(products_data, sector)
        result = self.client.generate(prompt, system=self.prompt_templates.system_prompt())
        
        if "error" in result or not result.get("response"):
            return self._fallback_summary(analyses, sector)
        
        # Nettoyer la r√©ponse
        summary = result["response"].strip()
        
        # Retirer les balises JSON si pr√©sentes
        if summary.startswith('```json'):
            summary = summary.replace('```json', '').replace('```', '')
        if summary.startswith('{') and summary.endswith('}'):
            # Tenter d'extraire le texte du JSON
            try:
                import json
                data = json.loads(summary)
                if 'resume_executif' in data:
                    summary = data['resume_executif']
                elif 'summary' in data:
                    summary = data['summary']
                elif 'text' in data:
                    summary = data['text']
            except:
                pass
        
        # Nettoyer les balises markdown
        summary = summary.replace('**', '').replace('__', '')
        summary = summary.replace('```', '').replace('`', '')
        
        # Nettoyer les sauts de ligne excessifs
        summary = ' '.join(summary.split())
        
        # Limiter √† 250 mots max
        words = summary.split()
        if len(words) > 250:
            summary = ' '.join(words[:250]) + '...'
        
        return summary
    
    def _generate_recommendations(
        self, 
        analyses: List[ProductAnalysis], 
        sector: str
    ) -> List[str]:
        """G√©n√®re les recommandations avec Ollama"""
        products_data = [{'name': a.name} for a in analyses]
        
        prompt = self.prompt_templates.recommendations_prompt(products_data, sector)
        result = self.client.generate(prompt, system=self.prompt_templates.system_prompt())
        
        if "error" in result or not result.get("response"):
            return self._fallback_recommendations()
        
        try:
            response_text = result["response"].strip()
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            
            if json_start != -1 and json_end != 0:
                json_str = response_text[json_start:json_end]
                data = json.loads(json_str)
                recs = data.get('recommendations', [])
                if len(recs) >= 6:
                    return recs[:6]
        except:
            pass
        
        return self._fallback_recommendations()
    
    def _fallback_analysis(self, product: str, sector: str) -> ProductAnalysis:
        """Analyse de secours (simulation)"""
        import numpy as np
        seed = abs(hash(product)) % 10000
        np.random.seed(seed)
        
        return ProductAnalysis(
            name=product,
            market_share=round(np.random.uniform(5, 35), 2),
            price=round(np.random.uniform(100, 2000), 2),
            satisfaction=round(np.random.uniform(3.0, 4.8), 2),
            growth=round(np.random.uniform(-10, 40), 2),
            strengths=np.random.choice(swot_data.STRENGTHS, size=4, replace=False).tolist(),
            weaknesses=np.random.choice(swot_data.WEAKNESSES, size=3, replace=False).tolist(),
            opportunities=np.random.choice(swot_data.OPPORTUNITIES, size=4, replace=False).tolist(),
            threats=np.random.choice(swot_data.THREATS, size=3, replace=False).tolist(),
            positioning=f"Acteur dans le segment {sector}",
            target_audience=f"Public cible {sector}"
        )
    
    def _fallback_summary(self, analyses: List[ProductAnalysis], sector: str) -> str:
        """R√©sum√© de secours"""
        import numpy as np
        avg_growth = np.mean([a.growth for a in analyses])
        avg_satisfaction = np.mean([a.satisfaction for a in analyses])
        leader = max(analyses, key=lambda x: x.market_share)
        
        return (
            f"Le secteur {sector} montre une dynamique "
            f"{'positive' if avg_growth > 0 else 'contrast√©e'} avec une croissance moyenne "
            f"de {avg_growth:.1f}%. {leader.name} domine avec {leader.market_share:.1f}% "
            f"de parts de march√©. Satisfaction moyenne: {avg_satisfaction:.1f}/5."
        )
    
    def _fallback_recommendations(self) -> List[str]:
        """Recommandations de secours"""
        import numpy as np
        return np.random.choice(
            recommendations_data.RECOMMENDATIONS,
            size=6,
            replace=False
        ).tolist()