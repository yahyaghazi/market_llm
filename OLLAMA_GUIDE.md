# ü§ñ Guide Ollama - Market Study Generator

## üìã Table des Mati√®res

1. [Installation Ollama](#installation-ollama)
2. [Configuration](#configuration)
3. [Utilisation](#utilisation)
4. [Hyperparam√®tres](#hyperparam√®tres)
5. [Mod√®les Recommand√©s](#mod√®les-recommand√©s)
6. [Exemples](#exemples)
7. [D√©pannage](#d√©pannage)

---

## üöÄ Installation Ollama

### Windows

```powershell
# 1. T√©l√©charger Ollama
# Aller sur: https://ollama.com/download
# T√©l√©charger OllamaSetup.exe et l'installer

# 2. V√©rifier l'installation
ollama --version

# 3. D√©marrer le service (automatique normalement)
# Si besoin manuel:
ollama serve
```

### Linux / Mac

```bash
# Installation
curl -fsSL https://ollama.com/install.sh | sh

# V√©rifier
ollama --version

# D√©marrer (automatique normalement)
ollama serve
```

---

## üì• T√©l√©charger les Mod√®les

### DeepSeek-R1 (Recommand√©)

```bash
# Version 14B (14 milliards de param√®tres)
ollama pull gemma3:4b

# Version 7B (plus l√©g√®re, ~8GB RAM)
ollama pull deepseek-r1:7b

# Version 1.5B (tr√®s l√©g√®re, ~2GB RAM)
ollama pull deepseek-r1:1.5b
```

### Autres Mod√®les Recommand√©s

```bash
# Llama 3.1 (excellent pour analyses)
ollama pull llama3.1:8b
ollama pull llama3.1:70b  # Si GPU puissant

# Mistral (rapide et efficace)
ollama pull mistral:7b

# Qwen 2.5 (bon pour fran√ßais)
ollama pull qwen2.5:7b

# Gemma 2 (de Google)
ollama pull gemma2:9b
```

### V√©rifier les Mod√®les Install√©s

```bash
# Lister tous les mod√®les
ollama list

# Exemple de sortie:
# NAME                    ID              SIZE    MODIFIED
# gemma3:4b        a1b2c3d4        8.1GB   2 hours ago
# llama3.1:8b            e5f6g7h8        4.7GB   1 day ago
```

---

## ‚öôÔ∏è Configuration de l'Application

### 1. Installation des D√©pendances Python

```bash
# Activer l'environnement virtuel
.\venv\Scripts\Activate.ps1  # Windows
source venv/bin/activate     # Linux/Mac

# Installer (requests est la seule nouvelle d√©pendance)
pip install requests
```

### 2. Structure des Fichiers

```
market-study/
‚îú‚îÄ‚îÄ ollama_analyzer.py      # ‚ú® Nouveau module Ollama
‚îú‚îÄ‚îÄ app_ollama.py           # ‚ú® App Flask avec Ollama
‚îú‚îÄ‚îÄ config.py               # Configuration
‚îú‚îÄ‚îÄ models.py               # Mod√®les de donn√©es
‚îú‚îÄ‚îÄ charts.py               # Graphiques
‚îú‚îÄ‚îÄ pdf_generator.py        # G√©n√©ration PDF
‚îî‚îÄ‚îÄ requirements.txt        # D√©pendances
```

### 3. D√©marrer l'Application

```bash
# Option 1: Utiliser le nouveau fichier
python app_ollama.py

# Option 2: Renommer et utiliser
cp app_ollama.py app.py
python app.py
```

---

## üéõÔ∏è Hyperparam√®tres Expliqu√©s

### Temperature (0.0 - 2.0)

**Contr√¥le la cr√©ativit√©/randomness des r√©ponses**

```python
# Temperature = 0.0 ‚Üí D√©terministe, r√©p√©table
{
  "ollama": {
    "temperature": 0.0
  }
}
# Parfait pour: Analyses factuelles, donn√©es pr√©cises

# Temperature = 0.7 ‚Üí √âquilibr√© (D√âFAUT)
{
  "ollama": {
    "temperature": 0.7
  }
}
# Parfait pour: Usage g√©n√©ral, bon mix cr√©ativit√©/pr√©cision

# Temperature = 1.5 ‚Üí Tr√®s cr√©atif
{
  "ollama": {
    "temperature": 1.5
  }
}
# Parfait pour: Recommandations innovantes, brainstorming
```

**Recommandations:**
- Analyses financi√®res: `0.2 - 0.4`
- Analyses de march√©: `0.6 - 0.8` ‚úÖ **Recommand√©**
- Recommandations strat√©giques: `0.8 - 1.2`

### Top-P / Nucleus Sampling (0.0 - 1.0)

**Contr√¥le la diversit√© du vocabulaire**

```python
# Top-P = 0.5 ‚Üí Vocabulaire restreint, conservateur
{
  "ollama": {
    "top_p": 0.5
  }
}

# Top-P = 0.9 ‚Üí Bon √©quilibre (D√âFAUT)
{
  "ollama": {
    "top_p": 0.9
  }
}
# ‚úÖ Recommand√© pour usage g√©n√©ral

# Top-P = 1.0 ‚Üí Vocabulaire complet
{
  "ollama": {
    "top_p": 1.0
  }
}
```

### Max Tokens (100 - 4000+)

**Longueur maximale de la r√©ponse**

```python
# Court
{
  "ollama": {
    "max_tokens": 500
  }
}
# Usage: R√©sum√©s courts

# Moyen (D√âFAUT)
{
  "ollama": {
    "max_tokens": 2000
  }
}
# ‚úÖ Recommand√©: Analyses compl√®tes

# Long
{
  "ollama": {
    "max_tokens": 4000
  }
}
# Usage: Analyses tr√®s d√©taill√©es
```

### Top-K (1 - 100)

**Limite le nombre de tokens candidats**

```python
{
  "ollama": {
    "top_k": 40  # D√©faut
  }
}
```

- `top_k: 10` ‚Üí Tr√®s conservateur
- `top_k: 40` ‚Üí √âquilibr√© ‚úÖ **Recommand√©**
- `top_k: 100` ‚Üí Tr√®s diversifi√©

### Repeat Penalty (1.0 - 2.0)

**P√©nalise la r√©p√©tition de mots**

```python
{
  "ollama": {
    "repeat_penalty": 1.1  # D√©faut
  }
}
```

- `1.0` ‚Üí Pas de p√©nalit√©
- `1.1` ‚Üí L√©ger ‚úÖ **Recommand√©**
- `1.5+` ‚Üí Fort (peut devenir incoh√©rent)

---

## üéØ Mod√®les Recommand√©s par Usage

### Pour Qualit√© Maximale

**gemma3:4b** + Configuration:
```json
{
  "ollama": {
    "model": "gemma3:4b",
    "temperature": 0.7,
    "top_p": 0.9,
    "max_tokens": 2500
  }
}
```

**Avantages:**
- Analyses tr√®s pr√©cises
- Excellent en fran√ßais
- Bon raisonnement

**Inconv√©nients:**
- N√©cessite ~16GB RAM
- Plus lent (~30-60s par produit)

### Pour Rapidit√©

**Llama 3.1:8b** + Configuration:
```json
{
  "ollama": {
    "model": "llama3.1:8b",
    "temperature": 0.7,
    "top_p": 0.9,
    "max_tokens": 2000
  }
}
```

**Avantages:**
- Rapide (~10-20s par produit)
- N√©cessite ~8GB RAM
- Bon √©quilibre qualit√©/vitesse

### Pour Machine L√©g√®re

**DeepSeek-R1:1.5b** + Configuration:
```json
{
  "ollama": {
    "model": "deepseek-r1:1.5b",
    "temperature": 0.8,
    "top_p": 0.95,
    "max_tokens": 1500
  }
}
```

**Avantages:**
- Tr√®s rapide (~5-10s)
- N√©cessite seulement ~2GB RAM
- Fonctionne sans GPU

**Inconv√©nients:**
- Qualit√© moindre
- Moins de nuance

---

## üìù Exemples d'Utilisation

### Exemple 1: Analyse Standard

```bash
curl -X POST http://localhost:5000/api/analyze \
  -H "Content-Type: application/json" \
  -d '{
    "products": ["iPhone 15", "Samsung Galaxy S24"],
    "sector": "Smartphones Premium",
    "ollama": {
      "use_ollama": true,
      "model": "gemma3:4b",
      "temperature": 0.7,
      "top_p": 0.9
    }
  }'
```

### Exemple 2: Analyse Cr√©ative

```bash
curl -X POST http://localhost:5000/api/analyze \
  -H "Content-Type: application/json" \
  -d '{
    "products": ["Tesla Model 3", "BMW i4"],
    "sector": "V√©hicules √âlectriques",
    "ollama": {
      "model": "gemma3:4b",
      "temperature": 1.2,
      "top_p": 0.95,
      "max_tokens": 3000
    }
  }'
```

### Exemple 3: Analyse Factuelle (D√©terministe)

```bash
curl -X POST http://localhost:5000/api/analyze \
  -H "Content-Type: application/json" \
  -d '{
    "products": ["Product A", "Product B"],
    "sector": "Technology",
    "ollama": {
      "model": "llama3.1:8b",
      "temperature": 0.2,
      "top_p": 0.8,
      "seed": 42
    }
  }'
```

### Exemple 4: Mode Fallback (Sans Ollama)

```bash
curl -X POST http://localhost:5000/api/analyze \
  -H "Content-Type: application/json" \
  -d '{
    "products": ["Product X", "Product Y"],
    "sector": "Market",
    "ollama": {
      "use_ollama": false
    }
  }'
```

### Exemple Python

```python
import requests

# Configuration
data = {
    "products": ["MacBook Pro M3", "Dell XPS 15", "ThinkPad X1"],
    "sector": "Laptops Professionnels",
    "ollama": {
        "use_ollama": True,
        "model": "gemma3:4b",
        "temperature": 0.7,
        "top_p": 0.9,
        "max_tokens": 2000
    }
}

# Appel API
response = requests.post(
    'http://localhost:5000/api/analyze',
    json=data,
    timeout=300  # 5 minutes max
)

# R√©sultat
if response.status_code == 200:
    result = response.json()
    print(f"‚úÖ PDF g√©n√©r√©: {result['pdf_filename']}")
    print(f"üìä {result['analysis']['products_count']} produits analys√©s")
    print(f"ü§ñ Mod√®le utilis√©: {result['analysis']['model']}")
else:
    print(f"‚ùå Erreur: {response.json()}")
```

---

## üîç V√©rification et Tests

### 1. Tester la Connexion Ollama

```bash
# Endpoint health
curl http://localhost:5000/health

# R√©ponse attendue:
{
  "status": "healthy",
  "version": "2.1.0-ollama",
  "ollama": {
    "available": true,
    "default_model": "gemma3:4b"
  }
}
```

### 2. Lister les Mod√®les Disponibles

```bash
curl http://localhost:5000/ollama/models

# R√©ponse:
{
  "success": true,
  "models": [
    "gemma3:4b",
    "llama3.1:8b",
    "mistral:7b"
  ],
  "count": 3
}
```

### 3. Test Complet

```bash
# Lancer l'app
python app_ollama.py

# Dans un autre terminal
python test_ollama.py  # Script de test (√† cr√©er)
```

---

## üêõ D√©pannage

### Probl√®me: "Ollama non accessible"

**Causes possibles:**
1. Ollama n'est pas d√©marr√©
2. Mauvais port
3. Firewall bloque

**Solutions:**
```bash
# V√©rifier si Ollama tourne
curl http://localhost:11434/api/tags

# D√©marrer Ollama
ollama serve

# V√©rifier le port (par d√©faut: 11434)
# Dans ollama_analyzer.py, modifier si n√©cessaire:
host: str = "http://localhost:11434"
```

### Probl√®me: "Mod√®le non trouv√©"

**Solution:**
```bash
# T√©l√©charger le mod√®le
ollama pull gemma3:4b

# V√©rifier
ollama list
```

### Probl√®me: "Timeout apr√®s 120s"

**Causes:**
- Mod√®le trop gros pour votre machine
- Pas de GPU, traitement lent

**Solutions:**
```python
# 1. Augmenter le timeout dans ollama_analyzer.py
timeout: int = 300  # 5 minutes

# 2. Utiliser un mod√®le plus l√©ger
"model": "deepseek-r1:7b"  # Au lieu de 14b

# 3. R√©duire max_tokens
"max_tokens": 1000  # Au lieu de 2000
```

### Probl√®me: "Erreur parsing JSON"

**Cause:** Le LLM ne g√©n√®re pas un JSON valide

**Solution automatique:** Le syst√®me utilise d√©j√† le fallback

**Solution manuelle:** Ajuster temperature
```python
# Plus bas = plus structur√©
"temperature": 0.3
```

### Probl√®me: "Out of Memory"

**Solutions:**
```bash
# 1. Utiliser un mod√®le plus petit
ollama pull deepseek-r1:1.5b

# 2. Fermer autres applications
# 3. V√©rifier RAM disponible

# Windows
wmic OS get FreePhysicalMemory

# Linux
free -h
```

---

## üìä Comparaison des Mod√®les

| Mod√®le | Taille | RAM Requise | Vitesse | Qualit√© | Fran√ßais |
|--------|--------|-------------|---------|---------|----------|
| **gemma3:4b** | 8.1GB | 16GB | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **deepseek-r1:7b** | 4.1GB | 8GB | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **llama3.1:8b** | 4.7GB | 8GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **mistral:7b** | 4.1GB | 8GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **qwen2.5:7b** | 4.4GB | 8GB | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **gemma2:9b** | 5.4GB | 10GB | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |

**L√©gende:**
- ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent
- ‚≠ê‚≠ê‚≠ê‚≠ê Tr√®s bon
- ‚≠ê‚≠ê‚≠ê Bon
- ‚≠ê‚≠ê Moyen

---

## üéì Bonnes Pratiques

### 1. Configuration par D√©faut (Recommand√©e)

```json
{
  "ollama": {
    "model": "gemma3:4b",
    "temperature": 0.7,
    "top_p": 0.9,
    "max_tokens": 2000,
    "top_k": 40,
    "repeat_penalty": 1.1
  }
}
```

### 2. Pour Donn√©es Financi√®res

```json
{
  "ollama": {
    "model": "gemma3:4b",
    "temperature": 0.3,
    "top_p": 0.8,
    "max_tokens": 1500
  }
}
```

### 3. Pour Brainstorming

```json
{
  "ollama": {
    "model": "llama3.1:8b",
    "temperature": 1.2,
    "top_p": 0.95,
    "max_tokens": 3000
  }
}
```

### 4. Pour Tests Rapides

```json
{
  "ollama": {
    "model": "deepseek-r1:1.5b",
    "temperature": 0.7,
    "max_tokens": 1000
  }
}
```

---

## üìö Ressources Suppl√©mentaires

- **Ollama Docs:** https://github.com/ollama/ollama/blob/main/docs/api.md
- **DeepSeek:** https://ollama.com/library/deepseek-r1
- **Llama 3.1:** https://ollama.com/library/llama3.1
- **Mistral:** https://ollama.com/library/mistral

---

## ‚úÖ Checklist de D√©marrage

- [ ] Ollama install√© (`ollama --version`)
- [ ] Mod√®le t√©l√©charg√© (`ollama pull gemma3:4b`)
- [ ] Ollama d√©marr√© (`ollama serve` ou automatique)
- [ ] Fichier `ollama_analyzer.py` copi√©
- [ ] Fichier `app_ollama.py` copi√©
- [ ] Application lanc√©e (`python app_ollama.py`)
- [ ] Health check OK (`curl localhost:5000/health`)
- [ ] Mod√®les list√©s (`curl localhost:5000/ollama/models`)
- [ ] Premier test r√©ussi

---

**üéâ Vous √™tes pr√™t √† utiliser DeepSeek-R1 pour vos √©tudes de march√© !**

*Market Study Generator v2.1 - Ollama Edition*